{"cells":[{"cell_type":"code","source":["Churn Analysis with Pyspark\n\nThis tutorial is created on the databricks cloud. It presents six classifiers that will be compared at the cross validation part. I will explain how to compute the different evaluate metrics on the binary classification case.\nChurn prediction is big business. It minimizes customer defection by predicting which customers are likely to cancel a subscription to a service. Though originally used within the telecommunications industry, it has become common practice across banks, ISPs, insurance firms, and other verticals.\nThe prediction process is heavily data driven and often utilizes advanced machine learning techniques. In this post, we will take a look at what types of customer data are typically used, do some preliminary analysis of the data, and generate churn prediction models - all with PySpark and its machine learning frameworks.  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30fc20b7-5250-4c9a-9e6b-3e2e194e6805"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nfrom sklearn.metrics import confusion_matrix\nsns.set_style(\"darkgrid\")\nimport pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92e5d350-b5aa-4152-9787-2b65583618d6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Initializing a Spark session\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Churn Prediction with PySpark\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"957d76f1-66d3-4740-bdbd-56f79db95da5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Importing Churn Data\n\nWe will be using the Orange Telecoms Churn Dataset. It consists of cleaned customer activity data (features), along with a churn label specifying whether the customer canceled their subscription or not. The two sets are from the same batch but have been split by an 80/20 ratio. We will use the larger set for training and cross-validation purposes, and the smaller set for final testing and model performance evaluation.\nIn order to read the CSV data and parse it into Spark Data Frames, we will use the CSV package.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61beed7b-b160-4cb1-b997-df2f817d430d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv('/FileStore/tables/churn_bigml_80-7.csv', header='true', inferSchema='true')\n\ntestdf = spark.read.csv('/FileStore/tables/churn_bigml_20-4.csv', header='true', inferSchema='true')\n\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"403c296b-dde1-4d37-9c71-188407fb4201"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1186ee4c-64ba-4026-8ff8-da67e3bc6579"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"The training dataset contains {} samples.\".format(df.count()))\nprint(\"The test dataset contains {} samples.\".format(testdf.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5159f969-2f1f-4460-8457-14f91729c14b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Generating a Pandas Dataframe with df, we can get a display of what the rows look like. We are using Pandas instead of the Spark DataFrame.show() function because it creates a clear picture of the dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc7f8774-c69b-4c7d-9279-bd6b9baba93b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.select(\"*\").toPandas().head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1df85ec2-a043-409c-b0f5-5565df289ff7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.select(\"*\").toPandas().info()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1732fac2-ab3c-4f8f-8a59-9b82c8c215fa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Summary Statistics\ndf.select(\"*\").toPandas().describe().transpose()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b012bbc2-bb8c-48b6-8f09-6a3382e1650e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Correlations and Data Preparation\nWe can also perform our own statistical analyses, using the seaborn package or other python packages. Indeed, we are use this library to examine correlations between the numeric columns by generating scatter plots of them."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Exploratory data analysis","showTitle":true,"inputWidgets":{},"nuid":"81fcda0b-afd5-4439-9c5f-2b0b17c599cb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Visualizing churn \nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(6,6), dpi=100)\nax = sns.countplot(x=\"Churn\", data=df.select(\"*\").toPandas())\nax.set_title('Distriburion of the Target Variable', fontsize=15)\nax.set_xlabel('Churn', fontsize = 15)\nax.set_ylabel('Count', fontsize = 15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13af2435-a1c4-4cb6-93f0-b4afcaed3791"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Convert (Churn)binary variable into numeric so plotting is easier. \ndf.select(\"*\").toPandas()['Churn'] = df.select(\"*\").toPandas()['Churn'].map({'Yes': 1, 'No': 0})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"773493ef-ddad-4217-9ee5-8409a87574b6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Various plots of Target (Churn) with different variables. This will provide an overview of dependancy of variables with target.\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(ncols=2, nrows=2, figsize=(27,31), dpi = 50)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None ,wspace=None, hspace=0.1)\nplt.rc('xtick', labelsize = 12)    # fontsize of the tick labels\nplt.rc('ytick', labelsize = 12)  \n\ndf.select(\"*\").toPandas().groupby('Area code').Churn.sum().plot(kind='bar', ax = ax1)\nax1.set_ylabel('Churn',fontsize = 10)\nax1.set_xlabel('Area code',fontsize = 15)\nax1.tick_params(labelsize = 18)\nax1.set_title('Churn count by Area code',fontsize = 15)\n\ndf.select(\"*\").toPandas().groupby('State').Churn.sum().plot(kind='bar', ax=ax2)\nax2.set_ylabel('Churn',fontsize = 10)\nax2.set_xlabel('State Type',fontsize = 15)\nax2.tick_params(labelsize = 12)\nax2.set_title('Churn count by State',fontsize = 15)\n\ndf.select(\"*\").toPandas().groupby('Total intl calls').Churn.sum().plot(kind='bar', ax=ax3)\nax3.set_ylabel('Churn',fontsize = 10)\nax3.set_xlabel('Total intl calls',fontsize = 15)\nax3.tick_params(labelsize = 18)\nax3.set_title('Churn count by Total intl call',fontsize = 15)\n\ndf.select(\"*\").toPandas().groupby('International plan').Churn.sum().plot(kind='bar', ax=ax4)\nax4.set_ylabel('Churn',fontsize = 10)\nax4.set_xlabel('International plan',fontsize = 15)\nax4.tick_params(labelsize = 18)\nax4.set_title('Churn count by International plan',fontsize = 15)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66a0670c-11b1-40a5-af44-66fca341b8a3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Converting to NaN since we need to find the mean and that cannot be computed with an empty string. It can be computed using NaN\n\nimport numpy as np\ndf.select(\"*\").toPandas().replace(\" \", np.nan, inplace=True) \ntestdf.select(\"*\").toPandas().replace(\" \", np.nan, inplace=True) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9f67051-7019-45e7-9aa4-61c1105ee020"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.select(\"*\").toPandas().isna().sum()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccbdef37-e14b-48cf-b247-c9b7ab485653"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Bar graph for Comparsion of Total day charge for each Area code with respect to target\n\nplt.figure(figsize=(40,8), dpi=80)\n# Sns.set(rc={'figure.figsize':(25,15)})\nax = sns.catplot(x=\"Area code\", y=\"Total day charge\", hue=\"Churn\", kind=\"box\", data=df.select(\"*\").toPandas(), height = 6,aspect = 1.5,palette = 'RdBu')\nplt.title('Comparsion of Total day charge for each Area code',fontsize = 20)\nplt.xlabel('Area code',fontsize = 15)\nplt.ylabel('Total day charge',fontsize = 15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a62d275c-e35c-4b78-9fcf-cb99381e6368"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Grouping Tenure data whether customers churn or not with respect to total day minutes using density plots\n\nkde_plot_data_a = df.select(\"*\").toPandas()[df.select(\"*\").toPandas()['Churn'] == 1]['Total day minutes']\nkde_plot_data_b = df.select(\"*\").toPandas()[df.select(\"*\").toPandas()['Churn'] == 0]['Total day minutes']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3968186-48c3-49c6-a568-6a9fa84035dc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["plt.figure(figsize=(10,6), dpi=80)\nsns.kdeplot(data = kde_plot_data_a,shade = True, legend = False)\nsns.kdeplot(data = kde_plot_data_b, shade = True, legend = False)\nplt.legend(title='Churning?', loc='upper right', labels=['Churn = Yes', 'Churn = No'])\nplt.title('Distribution of the Total day minutes for Churn = Yes|No',fontsize=20)\nplt.xlabel('Total day minutes',fontsize = 15)\nplt.ylabel('Kernel Density Estimate',fontsize = 15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2ff2158-0fde-4810-b7e7-dde23b81992f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Grouping Tenure data by whether customers churn or not with respect to total day calls\n\nkde_plot_data_c = df.select(\"*\").toPandas()[df.select(\"*\").toPandas()['Churn'] == 1]['Total day calls']\nkde_plot_data_d = df.select(\"*\").toPandas()[df.select(\"*\").toPandas()['Churn'] == 0]['Total day calls']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f94e9565-4314-40b6-84af-ef70248ffb46"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["plt.figure(figsize=(10,6), dpi=80)\nsns.kdeplot(data = kde_plot_data_c,shade = True, legend = False)\nsns.kdeplot(data = kde_plot_data_d, shade = True, legend = False)\nplt.legend(title='Churning?', loc='upper right', labels=['Churn = Yes', 'Churn = No'])\nplt.title('Distribution of the Total day calls for Churn = Yes|No',fontsize=20)\nplt.xlabel('Total day calls',fontsize = 15)\nplt.ylabel('Kernel Density Estimate',fontsize = 15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc8a6ef2-eda1-441c-84fc-13ebca1f03a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Grouping Tenure data by whether customers churn or not with respect to account length\n\nkde_plot_data_e = df.select(\"*\").toPandas()[df.select(\"*\").toPandas()['Churn'] == 1]['Total intl charge']\nkde_plot_data_f = df.select(\"*\").toPandas()[df.select(\"*\").toPandas()['Churn'] == 0]['Total intl charge']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e8a6ba5-7435-46dc-95ec-daf45bff2a0e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["plt.figure(figsize=(10,6), dpi=80)\nsns.kdeplot(data = kde_plot_data_e,shade = True, legend = False)\nsns.kdeplot(data = kde_plot_data_f, shade = True, legend = False)\nplt.legend(title='Churning?', loc='upper right', labels=['Churn = Yes', 'Churn = No'])\nplt.title('Distribution of the Account length for Churn = Yes|No',fontsize=20)\nplt.xlabel('Account length',fontsize = 15)\nplt.ylabel('Kernel Density Estimate',fontsize = 15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3076c95-e2f8-40d1-8f6f-50a9f23ec3d6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Converting test data from pyspark to panda dataframe\ntestdf.select(\"*\").toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"430b254c-0259-47d3-90fb-033d6e6638f2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Plot the heatmap to check the correlation\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (25,10))\nsns.heatmap(df.select(\"*\").toPandas().corr(), cmap = 'Blues', annot = True, center=0, fmt = '.2f')\n#plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fbc0785-a1a6-4c2b-bfc5-7ba0d5f0b146"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["It can be seen that there is good correlation between Total minutes and Total charge. Therefore, it is better to drop one of the can be dropped."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7c1c5ef-bce0-4b3f-9b62-04eaf8e8e409"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["We transform the categorical data into numeric as required by the machine learning routines, using a simple user-defined function that maps Yes/True and No/False to 1 and 0, respectively. All these tasks will be done using the following get_data function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a8affa0-4a40-490a-9104-5e6067d9b4e3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_data(df, removeCols):\n    df = df.drop(*removeCols) \\\n        .withColumn(\"Churn\", when(df[\"Churn\"] == 'true', 1.0).otherwise(0.0)) \\\n        .withColumn('International plan', when(df[\"International plan\"] == 'Yes', 1.0).otherwise(0.0)) \\\n        .withColumn('Voice mail plan', when(df[\"Voice mail plan\"] == 'Yes', 1.0).otherwise(0.0))\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9183723d-7905-4bf8-a300-36bf18e7c252"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import split, col, round, when\n\nremoveCols = ['State', 'Area code', 'Total day minutes', 'Total eve minutes', 'Total night minutes', 'Total intl minutes']\n\ntraindf = get_data(df, removeCols=removeCols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14af5e42-2b5e-4118-87c0-d7587ccc5ca5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# removing columns from test data as well \n\ntestdf = get_data(testdf, removeCols=removeCols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79fbeaa7-494d-499f-9d40-ea809d45a7d0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["testdf.select(\"*\").toPandas().info()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c755cfa-7bdb-4c92-aa69-777419f236d2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["testdf.select(\"*\").toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e69242f2-ad94-4288-a37a-d2674df21c6c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["traindf.select(\"*\").toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d61f4f3c-de3f-4574-a9bb-5ab174ffbec3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Using the Spark MLlib Package\n\nThe MLlib package provides a variety of machine learning algorithms for classification, regression, cluster and dimensionality reduction, as well as utilities for model evaluation. The decision tree is a popular classification algorithm, and we will be using extensively in this part.\n\nModel Training\n\nMLlib classifiers and regressors require data sets in a format of rows of type LabeledPoint, which separates row labels and feature lists, and names them accordingly. The custom labelData() function shown below performs the row parsing. We will pass it the prepared data set (CV_data) and split it further into training and testing sets. A decision tree classifier model is then generated using the training data. The tree depth can be regarded as an indicator of model complexity.\nNote: you can't map a dataframe, but you can convert the dataframe to an RDD and map that by doing data.rdd.map()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8177989-ec05-4a79-8d62-52afb7a264c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import DecisionTree\n\ndef labelData(data):\n     #label: row[end], features: row[0:end-1]\n    return data.rdd.map(lambda row: LabeledPoint(row[-1], row[:-1]))\n\ntraining_data, testing_data = labelData(traindf).randomSplit([0.8, 0.2])\n\nprint(\"The two first rows of the training data RDD:\")\nprint(training_data.take(2))\nprint(\"============================\")\n\nmodel = DecisionTree.trainClassifier(training_data, numClasses=2, maxDepth=2,\n                                     categoricalFeaturesInfo={1:2, 2:2},\n                                      maxBins=32, impurity='gini')\nprint(model.toDebugString())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d259bfe-8b81-43fa-a2ed-c79754c2159d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["The toDebugString() function provides a print of the tree's decision nodes and final prediction outcomes at the end leafs. We can see that features 12 and 4 are used for decision making and should thus be considered as having high predictive power to determine a customer's likeliness to churn. It's not surprising that these feature numbers map to the fields Customer service calls and Total day minutes. Decision trees are often used for feature selection because they provide an automated mechanism for determining the most important features (those closest to the tree root)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48f56ff8-1c71-45d3-80e7-db38c2a59f62"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Model Evaluation\n\nFirstly let’s expose a brief state of art about the Spark MLlib and ML functions that used to compute these metrics:\nMulticlassMetrics: from this function, it is possible to compute the confusion matrix, precision per class, weighted precision, recall per class, weighted recall, fmesure or score per class, weighted fmesure and accuracy.\nBinaryClassificationEvaluator: with this function, it is possible to compute two metrics which are the area under ROC and the area under PR.\nMulticlassClassificationEvaluator: according to this function, it is able to compute f1, weighted precision, weighted recall and accuracy.\nDue to the conflict that can be conducted by studying the different metrics, I will define in this subsection a printAllMetrics function in which I well explained how to compute the more known binary classification metrics.\nNote:\nThe new defined function needs a dataframe as input with essentially two required columns which are the target or label and the predicted label.\nWe have preceded some metrics by two stars (**) because they are the useful ones that are genarally used for the analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12d03f46-9715-41f1-b35d-dab9c3a27f4b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def printAllMetrics(predictionsDF):\n    TP = predictionsDF.select(\"label\", \"predictedLabel\").filter(\"label == 1.0 and predictedLabel == 1.0\").count()\n    FN = predictionsDF.select(\"label\", \"predictedLabel\").filter(\"label == 1.0 and predictedLabel == 0.0\").count()\n    TN = predictionsDF.select(\"label\", \"predictedLabel\").filter(\"label == 0.0 and predictedLabel == 0.0\").count()\n    FP = predictionsDF.select(\"label\", \"predictedLabel\").filter(\"label == 0.0 and predictedLabel == 1.0\").count()\n\n    precision_p   = TP/(TP+FP)\n    precision_n   = TN/(TN+FN)\n    recall_p      = TP/(TP+FN)\n    recall_n      = TN/(TN+FP)\n    f1_p          = 2*precision_p*recall_p/(precision_p+recall_p)\n    f1_n          = 2*precision_n*recall_n/(precision_n+recall_n)\n    avg_precision = (precision_p*(TP+FN)+precision_n*(TN+FP))/(TP+FN+TN+FP)\n    avg_recall    = (recall_p*(TP+FN)+recall_n*(TN+FP))/(TP+FN+TN+FP)\n    avg_f1        = (f1_p*(TP+FN)+f1_n*(TN+FP))/(TP+FN+TN+FP)\n    accuracy      = (TN+TP)/(TP+FN+TN+FP)\n\n    print('Precision of True    ', precision_p)\n    print('Precision of False   ', precision_n)\n    print('** Avg Precision     ', avg_precision)\n    print('Recall of True       ', recall_p)\n    print('Recall of False      ', recall_n)\n    print('** Avg Recall        ', avg_recall)\n    print('F1 of True           ', f1_p)\n    print('F1 of False          ', f1_n)\n    print('** Avg F1            ', avg_f1)\n    print('** Accuracy          ', accuracy)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f8af785-9e7a-440e-8e15-1179146486fa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Predictions of the testing data's churn outcome are made with the model's predict function and grouped together with the actual churn label of each customer data using getPredictionsLabels function.\nThe MLlib's RandomForest will be used for the model evaluation by taking rows of (prediction, label) tuples as input. It provides its metrics which have been bundled for printing with the custom printMetrics function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2626e82f-c8c3-4b65-b462-348ce26acbbe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.tree import RandomForest\nfrom time import *\n\nRF_NUM_TREES = 3\nRF_MAX_DEPTH = 4\nRF_NUM_BINS = 16\nRF_MAX_BINS = 227612\nstart_time = time()\n\nmodel = RandomForest.trainClassifier(training_data, \n                                     numClasses=2,\n                                     categoricalFeaturesInfo={}, \\\n                                     numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"gini\", \\\n                                     maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=45)\n\nend_time = time()\nelapsed_time = end_time - start_time\nprint(\"Time to train model: %.3f seconds\" % elapsed_time)\n\npredictions = model.predict(testing_data.map(lambda x: x.features))\nlabels_and_predictions = testing_data.map(lambda x: x.label).zip(predictions)\nacc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(testing_data.count())\nprint(\"Model precision: %.3f%%\" % (acc * 100))\n# model.save(sc, 'dbfs:/FileStore/RandomForestModel/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"448cddfc-bfb6-44ec-bb33-05ee70abd383"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Predictions of the testing data's churn outcome are made with the model's predict function and grouped together with the actual churn label of each customer data using getPredictionsLabels function.\nThe MLlib's MulticlassMetrics will be used for the model evaluation by taking rows of (prediction, label) tuples as input. It provides its metrics which have been bundled for printing with the custom printMetrics function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb4e36f5-0fe0-4310-a846-ba465473d740"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.evaluation import MulticlassMetrics\n\ndef getPredictionsLabels(model, testing_data):\n    predictions = model.predict(testing_data.map(lambda r: r.features))\n    return predictions.zip(testing_data.map(lambda r: r.label))\n\ndef printMetrics(predictions_and_labels):\n    metrics = MulticlassMetrics(predictions_and_labels)\n    print('Confusion Matrix\\n', metrics.confusionMatrix().toArray())\n    print('Precision of True    ', metrics.precision(1))\n    print('Precision of False   ', metrics.precision(0))\n    print('Weighted Precision   ', metrics.weightedPrecision)\n    print('Recall of True       ', metrics.recall(1))\n    print('Recall of False      ', metrics.recall(0))\n    print('Weighted Recall      ', metrics.weightedRecall) \n    print('FMeasure of True     ', metrics.fMeasure(1.0, 1.0))\n    print('FMeasure of False    ', metrics.fMeasure(0.0, 1.0))\n    print('Weighted fMeasure    ', metrics.weightedFMeasure())\n    print('Accuracy             ', metrics.accuracy)\n\npredictions_and_labels = getPredictionsLabels(model, testing_data)\n\nprintMetrics(predictions_and_labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09917bee-c745-480b-91b7-4e62888630ad"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["To check the above obtained results and comparing them with those that will be obtained using our new printAllMetrics function, let's displaying the confusion matrix that is used to compute all the variables of our new function:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a1d86a3-0c57-4a13-97cc-c47875358f01"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["predictionsdf = predictions_and_labels.toDF([\"predictedLabel\",\"label\"])\n\npredictionsdf.groupBy('label', 'predictedLabel').count().show()\nprint(\"========================================\")\n\nprintAllMetrics(predictionsdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c793e0e-c388-421c-8efd-f988ab54c47e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["As can be seen above, the FMesure given by the MulticlassMetrics is none other than the known classical F1 metric. All the metrics given by the printAllMetrics are correctly defined. Noting that the word Weighted is equivalent to Average.\n\nThe overall Accuracy, seems quite good, but one troubling issue is the discrepancy between the recall measures. The recall (aka sensitivity) for the Churn=False samples is high, while the recall for the Churn=True examples is relatively low. Business decisions made using these predictions will be used to retain the customers most likely to leave, not those who are likely to stay. Thus, we need to ensure that our model is sensitive to the Churn=True samples.\nPerhaps the model's sensitivity bias toward Churn=False samples is due to a skewed distribution of the two types of samples. Let's try grouping the CV_data DataFrame by the Churn field and counting the number of instances in each group."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3446a8e-13b0-4bd7-89a5-86569600847f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["traindf.groupby('Churn').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc5b0f03-8a38-4ab5-a815-b4fbf22b5a47"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Stratified Sampling"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bbfc732-3f84-4a1c-a198-165daf64cf27"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Stratified Sampling\nThere are roughly 6 times as many False churn samples as True churn samples. We can put the two sample types on the same footing using stratified sampling. The DataFrames sampleBy() function does this when provided with fractions of each sample type to be returned.\n\nHere we're keeping all instances of the Churn=True class, but downsampling the Churn=False class to a fraction of 388/2278."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30e0fe5f-b409-41e6-bf13-8fdf7600dca8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stratified_CV_data = traindf.sampleBy('Churn', fractions={0: 388./2278, 1: 1.0}).cache()\n\nstratified_CV_data.groupby('Churn').count().toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b95ed57b-0a61-4b9a-ad93-a37c7083a071"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Let's build a new model using the evenly distributed data set and see how it performs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23d862b0-3523-4d97-bc70-49e50d4f3213"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["training_data, testing_data = labelData(stratified_CV_data).randomSplit([0.8, 0.2])\n\nmodel = DecisionTree.trainClassifier(training_data, numClasses=2, maxDepth=2,\n                                     categoricalFeaturesInfo={1:2, 2:2},\n                                     impurity='gini', maxBins=32)\n\npredictions_and_labels = getPredictionsLabels(model, testing_data)\nprintMetrics(predictions_and_labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f5b811c-072b-4bec-a54c-c146a776e8db"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["With these new recall values, we can see that the stratified data was helpful in building a less biased model, which will ultimately provide more generalized and robust predictions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f40513a-7aff-4360-9467-54cb633fe621"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Using the Spark ML Package\n\nThe ML package is the newer library of machine learning routines. It provides an API for pipelining data transformers, estimators and model selectors. It will be used here to perform cross-validation across several classifiers in order to find the optimal model.\nPipelining\nThe ML package needs data be put in a (label: Double, features: Vector) DataFrame format with correspondingly named fields using the VectorAssembler function. The global get_dummy function defined below performs this formatting in which we will also pass the data through a pipeline of two transformers, StringIndexer and VectorIndexer which index the label and features fields respectively.\nModel Selection\nGiven the data set at hand, we would like to determine which parameter values of or classifier produce the best model. We need a systematic approach to quantatively measure the performance of the models and ensure that the results are reliable. This task of model selection is often done using cross validation techniques. A common technique is k-fold cross validation, where the data is randomly split into k partitions. Each partition is used once as the testing data set, while the rest are used for training. Models are then generated using the training sets and evaluated with the testing sets, resulting in k model performance measurements. The average of the performance scores is often taken to be the overall score of the model, given its build parameters.\nFor model selection, we can search through the model parameters, comparing their cross validation performances. The model parameters leading to the highest performance metric produce the best model.\nThe ML package supports k-fold cross validation, which can be readily coupled with a parameter grid builder and an evaluator to construct a model selection workflow. Below, we will use a transformation/estimation pipeline to train our classifier models. The cross validator will use in this example an empty ParamGridBuilder to iterate through the default parameters of the studied classifiers and evaluate their models using the F1 score, repeating 3 times per default parameter value for reliable results.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d510c25-d3d7-49fe-8f6e-e65051ef62d4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["We define a get_dummy function that transforms a given classical dataframe to a new other one composed of dense vectors reliable to be running with Spark ML."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17e31fb5-c5e0-41dc-aad5-0d9fda86cac0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\n\ndef get_dummy(df, numericCols, labelCol):\n    # Combining a given list of columns into a single vector column features\n    assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n\n    # Index labels, adding metadata to the label column\n    indexer = StringIndexer(inputCol=labelCol, outputCol='indexedLabel')\n\n    # Automatically identify categorical features and index them\n    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=2)\n\n    pipeline = Pipeline(stages = [assembler] + [indexer] + [featureIndexer])\n\n    model = pipeline.fit(df)\n    data = model.transform(df)\n\n    data = data.withColumn('label', col(labelCol))\n\n    return data.select('features', 'label', 'indexedFeatures', 'indexedLabel')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61cf2b90-33dd-4e19-8bf6-b07bf9d62ece"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Once the required function is ready, let's define the needed numericCols list by removing the Churn column and transforming the datasets:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9be0db00-5549-41ea-b688-155e6484f9ab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["numericCols = stratified_CV_data.columns\nnumericCols.remove(\"Churn\")\n\nvectorized_CV_data = get_dummy(traindf, numericCols, \"Churn\")\nvectorized_stratified_CV_data = get_dummy(stratified_CV_data, numericCols, \"Churn\")\nvectorized_final_test_data = get_dummy(testdf, numericCols, \"Churn\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ecd7f1d-a717-43ad-a01b-356a238ff73b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Let's now looking the structure of one of the transformed datasets to see what has been happened by comparing it to the structure displayed above:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68d7ddeb-35ee-4845-8ab5-e7f864c32b81"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["vectorized_stratified_CV_data.show(2, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cf2a435-71b9-420e-9ac5-7c883fe5864f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["K-fold cross validation\n\nI will define below a cvComparing function in order to compute the cross validation procedures and keeping all the necessary metrics during the training as well as the test steps. In order to make the results more readable, I rounded the metrics by specifying the number of decimal places."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d48a45fe-a620-4d62-8107-0a3fc585e14a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def cvComparing(vectorized_train_data, vectorized_test_data, classifiers, paramGrid, numFolds, roundLevel, seed):\n    names = []\n    underROC_train = []\n    underROC_test = []\n    f1_train = []\n    f1_test = []\n    wp_train = []\n    wp_test = []\n    wr_train = []\n    wr_test = []\n    acc_train = []\n    acc_test = []\n\n    evaluatorB = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"indexedLabel\")\n\n    evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\") \n    evaluatorwp = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\") \n    evaluatorwr = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\") \n    evaluatoracc = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\") \n\n    for name, clf in classifiers:\n        cv = CrossValidator(estimator=clf, estimatorParamMaps=paramGrid, evaluator=evaluatorf1, numFolds=numFolds, seed=seed, collectSubModels=True)\n        cvModel = cv.fit(vectorized_train_data)  \n        predict_train = cvModel.transform(vectorized_train_data)\n        predict_test = cvModel.transform(vectorized_test_data)\n        underROC_train.append(evaluatorB.evaluate(predict_train))\n        underROC_test.append(evaluatorB.evaluate(predict_test))\n        f1_train.append(evaluatoracc.evaluate(predict_train))\n        f1_test.append(evaluatoracc.evaluate(predict_test))\n        wp_train.append(evaluatorwp.evaluate(predict_train))\n        wp_test.append(evaluatorwp.evaluate(predict_test))\n        wr_train.append(evaluatorwr.evaluate(predict_train))\n        wr_test.append(evaluatorwr.evaluate(predict_test))\n        acc_train.append(evaluatoracc.evaluate(predict_train))\n        acc_test.append(evaluatoracc.evaluate(predict_test))\n        names.append(name)\n\n    cvResults = spark.createDataFrame(zip(names, underROC_train, underROC_test, acc_train, acc_test, f1_train, f1_test, wp_train, wp_test, wr_train, wr_test), \n                                       schema=['Classifier name', 'underROC_train', 'underROC_test', 'Accuracy_train', 'Accuracy_test', 'f1_train', 'f1_test', 'wPrecision_train', 'wPrecision_test', 'wRecall_train', 'wRecall_test'])\n  \n    for t in cvResults.dtypes:\n        if t[1] == 'double':\n            cvResults = cvResults.withColumn(t[0], round(cvResults[t[0]], roundLevel))\n  \n    return cvResults"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2ebae56-4425-48e2-a86d-26f3a2481c43"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression, NaiveBayes, LinearSVC, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = ParamGridBuilder().build()\n\nclassifiers = []\n##############################################\nclassifiers.append(('LR', LogisticRegression(labelCol='indexedLabel', featuresCol='indexedFeatures')))\nclassifiers.append(('NB', NaiveBayes(labelCol='indexedLabel', featuresCol='indexedFeatures')))\nclassifiers.append(('SVC', LinearSVC(labelCol='indexedLabel', featuresCol='indexedFeatures')))\nclassifiers.append(('DT', DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')))\nclassifiers.append(('RF', RandomForestClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')))\nclassifiers.append(('GBT', GBTClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')))\n##############################################\n\n\ncvResults_CV_data = cvComparing(vectorized_train_data=vectorized_CV_data, vectorized_test_data=vectorized_final_test_data, \n                                classifiers=classifiers, paramGrid=paramGrid, numFolds=5, roundLevel=3, seed=123)\n\ncvResults_stratified_CV_data = cvComparing(vectorized_train_data=vectorized_stratified_CV_data, \n                                           vectorized_test_data=vectorized_final_test_data, classifiers=classifiers, \n                                           paramGrid=paramGrid, numFolds=5, roundLevel=3, seed=123)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"362b13d8-d81e-4808-ab1f-10cb9c52ef25"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Model Evaluation\n\nThe actual performance of the models is determined based on both CV_data and stratified_CV_data as well as final_test_data sets. The last one has not been used for any training or cross validation activities. We will transform the test set with the model pipeline, which will map the labels and features according to the same recipe. The evaluators will provide us all the metrics of the MulticlassClassificationEvaluator function as well as the Area Under ROC (underRoc) metric based on the BinaryClassificationEvaluator obtained from the predictions during the training and the test steps. These metrics are computed using the above studied classifiers with its default parameters accross 5-fold cross validation by trying to find the best value of the F1 metric."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"420f22ff-267e-4adf-875f-6865a67f46ed"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Metrics computed using all the data of CV_data for the training step:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3985d26-5fd0-42ec-ae4f-d968526275db"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cvResults_CV_data.toPandas().set_index('Classifier name')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"403eb70e-ad39-4ec9-8f6e-b9262e1b452b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cvResults_CV_data.toPandas().transpose()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"279acbe8-2c39-4267-bcfc-d2bf90d00508"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(cvResults_CV_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2aaf4ed-f4bd-4f6e-92aa-b9afd100b8ca"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cvResults_stratified_CV_data.toPandas().set_index('Classifier name')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71e8988e-06d3-4858-a581-971000bd6a56"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cvResults_stratified_CV_data.toPandas().transpose()\ndisplay(cvResults_stratified_CV_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48d97b69-1aa1-4f0e-8eae-f9fe35ba0787"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["According to the above two plots, we cannot confirm that the data stratification is a good idea that can improve the learning. The metrics computed using all the data are higher than those obtained with the stratified data. This implies that the stratified data are poorly and cannot goodly cover all the situations present in global data. It is clear that the Decision tree, Random forest and Gradient-boosted tree are the more efficient classifiers. However, it seems that the Decision tree is the best classifier because all its metrics are close to each other for the two situations of data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4513d01d-0df4-42b2-8e26-ec9ca0705b1c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Comparison of the metric values given by the printAllMetrics function with those obtained using MulticlassClassificationEvaluator:\nTo achieve such a comparison, let's using one of the above studied classifier and computing the required metrics based on the two functions:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffdc85fe-6cc9-44cf-b515-13d80f0fa25e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\") \nevaluatorwp = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\") \nevaluatorwr = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\") \nevaluatorac = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\") \n\n\nclf = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')\n#clf = NaiveBayes(labelCol='indexedLabel', featuresCol='indexedFeatures')\n\nclfModel = clf.fit(vectorized_final_test_data)  \npred_train = clfModel.transform(vectorized_final_test_data)\n\nprint('Weighted Precision   ', evaluatorwp.evaluate(pred_train))\nprint('Weighted Recall      ', evaluatorwr.evaluate(pred_train))\nprint('F1                   ', evaluatorf1.evaluate(pred_train))\nprint('Accuracy             ', evaluatorac.evaluate(pred_train))\n\n\nprint(\"========================================\")\n\nprintAllMetrics(pred_train.withColumn('predictedLabel', col(\"prediction\")))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a95e0f9b-093d-4bf9-a522-81e11c3b25b8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["All the metrics given by the new printAllMetrics function are correctly running. The four metrics such as Avg Precision, Avg Recall, Avg F1 and Accuracy are exactly the same as those given by the MulticlassClassificationEvaluator function. It is noticed that the term Weighted has the sense of Average. F1 computed with MulticlassClassificationEvaluator is none other than the Avg F1 calculated with the printAllMetrics function or the Weighted FMeasure computed using the MulticlassMetrics function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"749fc69a-11d8-4bbe-82f3-c063712de14e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Conclusion:\n    \nThe Decision tree, Random forest and Gradient-boosted tree are the more efficient classifiers. However, the best model created according to the cross validation process seems to be the Decision tree. Indeed, the different metrics obtained by using this classifier are higher as well as very close to each other comparing to those of the other classifiers. An explication on how to compute all the different metrics excepting both area under ROC and area under PR is given in a function called printAllMetrics.\nOther improvements are still to be expected very soon as a prediction based on cross validation applying the ParamGridBuilder to iterate through required parameters according to each classifier."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb65cd71-8d07-4510-b1bb-cd429fcfcb90"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Orange-Telecom-churn-final (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":355671407037746}},"nbformat":4,"nbformat_minor":0}
